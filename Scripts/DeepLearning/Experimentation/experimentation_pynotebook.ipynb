{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-118d9ace962d>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-8-118d9ace962d>\"\u001B[1;36m, line \u001B[1;32m30\u001B[0m\n\u001B[1;33m    def create_train_window(input, label, group_size=6):\u001B[0m\n\u001B[1;37m      ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, LSTM\n",
    "\n",
    "import tensorflow.keras.losses as losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs.plots as plotting\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow_docs.plots as plotting\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Experimentation\n",
    "path_output = 'Experimentation/TwoGruTwoCategorical25Dropout'\n",
    "\n",
    "res_exp = 3\n",
    "experimentation_list = [[ recurrent_dropout, dropout_gru]\n",
    "                                  for recurrent_dropout in np.linspace(0.1, 0.4, res_exp)\n",
    "                                  for dropout_gru in np.linspace(0.1, 0.4, res_exp)]\n",
    "experimentation_list = [[0.25, 0.25]]\n",
    "# Main\n",
    "def create_train_window(input, label, group_size=6):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for i in range(0, len(input)-group_size):\n",
    "        week_volume = label[i:group_size+i]\n",
    "        max_week_volume = np.max(week_volume)\n",
    "        noVolume_prob = np.random.choice([True,False], p=[0.6,0.4])\n",
    "\n",
    "        if max_week_volume > 0 or noVolume_prob:\n",
    "            if(i < (len(input)-group_size-1)*0.8):\n",
    "                x_train.append(input[i:group_size+i])\n",
    "                y_train.append(label[i+group_size-1])\n",
    "            else:\n",
    "                x_test.append(input[i:group_size+i])\n",
    "                y_test.append(label[i+group_size-1])\n",
    "    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "\n",
    "\n",
    "df = pd.read_csv('../../OnlineData/CategoricalVolume_8cp.csv')\n",
    "\n",
    "\n",
    "# Adding to input One hot encoded Month and CP\n",
    "\n",
    "CP_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "CP_enc.fit(np.array(df.iloc[:, 1]).reshape(-1, 1))\n",
    "CP_OneHotEncoded = CP_enc.transform(np.array(df.iloc[:, 1]).reshape(-1, 1)).toarray()\n",
    "\n",
    "month_serie = list(map(lambda x: x.split('-')[1], np.array(df.iloc[:, 0])))\n",
    "Month_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Month_enc.fit(np.array(month_serie).reshape(-1, 1))\n",
    "Month_OneHotEncoded = Month_enc.transform(np.array(month_serie).reshape(-1, 1)).toarray()\n",
    "\n",
    "cp_transformed_df = pd.DataFrame(data=CP_OneHotEncoded, columns=CP_enc.categories_)\n",
    "\n",
    "categories_month = np.array(list(map(lambda x: f'M-{x}', Month_enc.categories_[0])))\n",
    "month_transformed_df = pd.DataFrame(data=Month_OneHotEncoded, columns=categories_month)\n",
    "\n",
    "\n",
    "# One hot encoding label\n",
    "label_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "label_enc.fit(np.array(df.iloc[:, 20]).reshape(-1, 1))\n",
    "label_OneHotEncoded = label_enc.transform(np.array(df.iloc[:, 20]).reshape(-1, 1)).toarray()\n",
    "\n",
    "label_transformed_df =  pd.DataFrame(data=label_OneHotEncoded, columns=label_enc.categories_)\n",
    "\n",
    "\n",
    "\n",
    "input = df.iloc[:, 2:20]\n",
    "label_transformed = np.array(label_transformed_df)\n",
    "\n",
    "input = cp_transformed_df.join(input)\n",
    "input = input.join(month_transformed_df)\n",
    "\n",
    "\n",
    "\n",
    "scaler_input = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "input_transformed = scaler_input.fit_transform(input)\n",
    "\n",
    "df_transformed = pd.DataFrame(input_transformed)\n",
    "\n",
    "\n",
    "group_size = 6\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "num_cps_unique = len(CP_OneHotEncoded[0])\n",
    "\n",
    "for cp in range(0, num_cps_unique):\n",
    "    input_t = np.array([])\n",
    "    label_t = np.array([])\n",
    "    for ind in range(0, len(input_transformed)):\n",
    "        if input_transformed[ind][cp] == 1:\n",
    "            input_t = np.append(input_t, input_transformed[ind])\n",
    "            label_t = np.append(label_t, label_transformed[ind])\n",
    "    input_t = input_t.reshape((-1, len(input_transformed[0])))\n",
    "    label_t = label_t.reshape((-1, len(label_transformed[0])))\n",
    "\n",
    "    x_train_t, y_train_t, x_test_t, y_test_t = create_train_window(input_t, label_t, group_size=group_size)\n",
    "\n",
    "    x_train = np.append(x_train, x_train_t)\n",
    "    y_train = np.append(y_train, y_train_t)\n",
    "    x_test = np.append(x_test, x_test_t)\n",
    "    y_test = np.append(y_test, y_test_t)\n",
    "\n",
    "x_train = x_train.reshape(-1, group_size, len(input_transformed[0]))\n",
    "y_train = y_train.reshape(-1, len(label_transformed[0]))\n",
    "x_test = x_test.reshape(-1, group_size, len(input_transformed[0]))\n",
    "y_test = y_test.reshape(-1, len(label_transformed[0]))\n",
    "\n",
    "input_shape_w = np.shape(x_train[0])\n",
    "\n",
    "\n",
    "# MODEL\n",
    "\n",
    "y_train_class = list(map(np.argmax, y_train))\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_class), y_train_class)\n",
    "class_weights = {id:class_weights[id] for id in range(0,len(y_train[0]))}\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "# Parametres\n",
    "LR = 0.001\n",
    "FACTOR_LR = 0.5\n",
    "MIN_LR = LR / 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model',\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    factor=FACTOR_LR,\n",
    "    min_lr=MIN_LR,\n",
    ")\n",
    "\n",
    "with open(f'{path_output}/ID_params.txt', 'w') as j:\n",
    "    j.write('ID: RECURRENT_DROPOUT, DROPOUT_GRU\\n')\n",
    "\n",
    "for ID, arr in enumerate(experimentation_list):\n",
    "    GRU_SIZE, DENSE_SIZE, DROP_OUT = [128, 20, 0.1]\n",
    "    RECURRENT_DROP, DROP_OUT_GRU = arr\n",
    "    # CreaciÃ³n MODELO\n",
    "    model = Sequential()\n",
    "    model.add(GRU(GRU_SIZE, input_shape=input_shape_w, return_sequences=True, recurrent_dropout=RECURRENT_DROP, dropout= DROP_OUT_GRU))\n",
    "    model.add(GRU(GRU_SIZE, recurrent_dropout=RECURRENT_DROP, dropout= DROP_OUT_GRU))\n",
    "    model.add(Dense(DENSE_SIZE, activation='relu'))\n",
    "    model.add(Dropout(DROP_OUT))\n",
    "    model.add(Dense(label_transformed_df.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=10000000 ,shuffle=True, batch_size=8,\n",
    "                        callbacks=[reduce_lr, early_stopping], validation_data=(x_test, y_test),\n",
    "                        class_weight=class_weights, verbose=1)\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "    # save to json:\n",
    "    with open(f'{path_output}/{ID}-hist.json', mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "    model.save(f'{path_output}/{ID}-model')\n",
    "\n",
    "    with open(f'{path_output}/ID_params.txt', 'a') as j:\n",
    "        j.write(f\"{str(ID)}:{','.join(list(map(str,arr)))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}